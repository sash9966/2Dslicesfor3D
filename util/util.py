"""
Copyright (C) 2019 NVIDIA Corporation.  All rights reserved.
Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).
"""
import matplotlib

matplotlib.use('Agg')
import SimpleITK as sitk
from scipy.ndimage import zoom
import pickle
import re
import importlib
import torch
from argparse import Namespace
import numpy as np
from PIL import Image
import os
import argparse
import dill as pickle
import util.coco
import torch
from typing import Optional
import random
import matplotlib.pyplot as plt
from torchvision.utils import make_grid , save_image
from models.networks import vae_loss
from util import util
import gc   
def one_hot(
    labels: torch.Tensor,
    num_classes: int,
    device: Optional[torch.device] = None,
    dtype: Optional[torch.dtype] = None,
    eps: float = 1e-6,
) -> torch.Tensor:
    r"""Converts an integer label x-D tensor to a one-hot (x+1)-D tensor.

    Args:
        labels: tensor with labels of shape :math:`(N, *)`, where N is batch size.
          Each value is an integer representing correct classification.
        num_classes: number of classes in labels.
        device: the desired device of returned tensor.
        dtype: the desired data type of returned tensor.

    Returns:
        the labels in one hot tensor of shape :math:`(N, C, *)`,

    Examples:
        >>> labels = torch.LongTensor([[[0, 1], [2, 0]]])
        >>> one_hot(labels, num_classes=3)
        tensor([[[[1.0000e+00, 1.0000e-06],
                  [1.0000e-06, 1.0000e+00]],
        <BLANKLINE>
                 [[1.0000e-06, 1.0000e+00],
                  [1.0000e-06, 1.0000e-06]],
        <BLANKLINE>
                 [[1.0000e-06, 1.0000e-06],
                  [1.0000e+00, 1.0000e-06]]]])

    """
    if not isinstance(labels, torch.Tensor):
        raise TypeError("Input labels type is not a torch.Tensor. Got {}".format(type(labels)))

    if not labels.dtype == torch.int64:
        raise ValueError("labels must be of the same dtype torch.int64. Got: {}".format(labels.dtype))

    if num_classes < 1:
        raise ValueError("The number of classes must be bigger than one." " Got: {}".format(num_classes))

    shape = labels.shape
    one_hot = torch.zeros((shape[0], num_classes) + shape[1:], device=device, dtype=dtype)

    return one_hot.scatter_(1, labels.unsqueeze(1), 1.0) + eps


def show_gen(generated, label=False, path='generated.png', title = 'synthesized'):
    plt.figure(figsize=(50,50))
    plt.axis("off")
    # plt.title("Images ")
    if label:
        plt.imshow(np.transpose(make_grid(generated, nrow=10, padding=2,    normalize=True).cpu(),(1,2,0)), cmap='hot')
        plt.title(title)
    else:
        plt.imshow(np.transpose(make_grid(generated, nrow=10, padding=2,    normalize=True).cpu(),(1,2,0)), cmap='jet')
        plt.title(title)
        # save_image(make_grid(generated,nrow=10, padding=2,    normalize=True).cpu(),fp=path)

### create tool for visualization
def print_current_errors(epoch, i, errors, t, out_dir):
    # create a logging file to store training losses
    log_name = os.path.join(out_dir, 'loss_log.txt')
    # with open(log_name, "a") as log_file:
    #         now = time.strftime("%c")
    #         log_file.write('================ Training Loss (%s) ================\n' % now)

    message = '(epoch: %d, iters: %d, time: %.3f) ' % (epoch, i, t)
    for k, v in errors.items():
        #print(v)
        #if v != 0:
        v = v.mean().float()
        message += '%s: %.3f ' % (k, v)
    
    print(message)
    with open(log_name, "a") as log_file:
            log_file.write('%s\n' % message)  # save the message

# dataloader for loading labels
def set_seed(seed):
    """Set all random seeds."""
    if seed is not None:
        np.random.seed(seed)
        random.seed(seed)
        torch.manual_seed(seed)
        # if want pure determinism could uncomment below: but slower
        # torch.backends.cudnn.deterministic = True
### ================================================================================ loss calculation functions starts
def combined_loss(reconstructed, output_image, mu, logvar, type = 'BCE', lamda_kld = 0.5, beta_dice = 1000 ):
    # choose between 'BCE'  'MSE' 'L1' and 'L1F'
    losses = {'KLD': [], 'Rec': [], 'comb':[]}
    if type == 'BCE':
        rec_loss = vae_loss.BCELoss()
    elif type == 'MSE':
        rec_loss = vae_loss.MSELoss()
    elif type == 'L1':
        rec_loss = vae_loss.L1Loss()
    elif type == 'L1F':
        rec_loss = vae_loss.L1FLoss()
    elif type == 'Dice':
        rec_loss = vae_loss.DiceLoss()
    elif type == 'CE':
        rec_loss = vae_loss.CELoss()
    elif type == 'CEDice':
        rec_loss1 = vae_loss.CELoss()
        rec_loss2 = vae_loss.DiceLoss()
    else:
        raise ValueError("Unkown reconstruction loss: {}".format(type))


    if type == 'CEDice':
        REC1 = rec_loss1(reconstructed, output_image) 
        REC2 = rec_loss2(reconstructed, output_image) 
        REC = REC1 + REC2 * beta_dice
        losses['Rec1']= REC1
        losses['Rec2']= REC2 * beta_dice
    else:
        REC = rec_loss(reconstructed, output_image)
    
    # kld_loss = vae_losses.KLDLoss()
    kld_loss = vae_loss.KLDALoss2()
    KLD = kld_loss(mu, logvar)
    combined = REC + lamda_kld * KLD
    losses['KLD']= KLD
    losses['Rec']= REC
    losses['comb']= combined
    return combined, losses

def combined_loss_beta_VAE(reconstructed, output_image, mu, logvar, type = 'CE', lamda_kld = 1, n_train_steps=1 ):

    if(torch.cuda.is_available()):

        output_image = output_image.cuda().type(torch.cuda.LongTensor).squeeze()

    else:
        output_image = output_image.type(torch.LongTensor).squeeze()
    
    # choose between 'BCE'  'MSE' 'L1' and 'L1F'
    losses = {'KLD': [], 'Rec': [], 'comb':[]}
    if type == 'BCE':
        rec_loss = vae_loss.BCELoss()
    elif type == 'MSE':
        rec_loss = vae_loss.MSELoss()
    elif type == 'L1':
        rec_loss = vae_loss.L1Loss()
    elif type == 'L1F':
        rec_loss = vae_loss.L1FLoss()
    elif type == 'Dice':
        rec_loss = vae_loss.DiceLoss()
    elif type == 'CE':
        rec_loss = vae_loss.CELoss()
    elif type == 'CEDice':
        rec_loss1 = vae_loss.CELoss()
        rec_loss2 = vae_loss.DiceLoss()
    else:
        raise ValueError("Unkown reconstruction loss: {}".format(type))

    
    REC = rec_loss(reconstructed, output_image)
    
    # kld_loss = vae_losses.KLDLoss()
    kld_loss = vae_loss.KLDALoss2()
    KLD = kld_loss(mu, logvar)
    is_train = True
    C_init = 0
    C_fin = 1
    steps_anneal = 4316 * 50 *2
    C = linear_annealing(C_init, C_fin, n_train_steps, steps_anneal)
    # combined = REC + lamda_kld * KLD
    # combined = REC + lamda_kld * (KLD - C).abs()
    combined = REC + lamda_kld * KLD 
    losses['KLD']= KLD
    losses['Rec']= REC
    losses['comb']= combined
    return combined, losses

def linear_annealing(init, fin, step, annealing_steps):
    """Linear annealing of a parameter."""
    if annealing_steps == 0:
        return fin
    assert fin > init
    delta = fin - init
    annealed = min(init + delta * step / annealing_steps, fin)
    return annealed

def load_network_vae(net, epoch, opt, device):
    model_dir = opt.web_dir if 'train' in opt.web_dir else opt.web_dir.replace('test', 'train')
    mode_name = model_dir + 'VAE_net' + str(epoch) + '.pth'
    print('loaded network {}'.format(mode_name))
    weights = torch.load(mode_name, map_location=device)
    net.load_state_dict(weights)
    return net




def set_seed(seed):
    """Set all random seeds."""
    if seed is not None:
        np.random.seed(seed)
        random.seed(seed)
        torch.manual_seed(seed)
        # if want pure determinism could uncomment below: but slower
        # torch.backends.cudnn.deterministic = True




def save_obj(obj, name):
    with open(name, 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)


def load_obj(name):
    with open(name, 'rb') as f:
        return pickle.load(f)

# returns a configuration for creating a generator
# |default_opt| should be the opt of the current experiment
# |**kwargs|: if any configuration should be overriden, it can be specified here


def copyconf(default_opt, **kwargs):
    conf = argparse.Namespace(**vars(default_opt))
    for key in kwargs:
        print(key, kwargs[key])
        setattr(conf, key, kwargs[key])
    return conf


def tile_images(imgs, picturesPerRow=4):
    """ Code borrowed from
    https://stackoverflow.com/questions/26521365/cleanly-tile-numpy-array-of-images-stored-in-a-flattened-1d-format/26521997
    """

    # Padding
    if imgs.shape[0] % picturesPerRow == 0:
        rowPadding = 0
    else:
        rowPadding = picturesPerRow - imgs.shape[0] % picturesPerRow
    if rowPadding > 0:
        imgs = np.concatenate([imgs, np.zeros((rowPadding, *imgs.shape[1:]), dtype=imgs.dtype)], axis=0)

    # Tiling Loop (The conditionals are not necessary anymore)
    tiled = []
    for i in range(0, imgs.shape[0], picturesPerRow):
        tiled.append(np.concatenate([imgs[j] for j in range(i, i + picturesPerRow)], axis=1))

    tiled = np.concatenate(tiled, axis=0)
    return tiled


# Converts a Tensor into a Numpy array
# |imtype|: the desired type of the converted numpy array
def tensor2im(image_tensor, imtype=np.uint8, normalize=True, tile=False):
    if isinstance(image_tensor, list):
        image_numpy = []
        for i in range(len(image_tensor)):
            image_numpy.append(tensor2im(image_tensor[i], imtype, normalize))
        return image_numpy

    if image_tensor.dim() == 4:
        # transform each image in the batch
        images_np = []
        for b in range(image_tensor.size(0)):
            one_image = image_tensor[b]
            one_image_np = tensor2im(one_image)
            images_np.append(one_image_np.reshape(1, *one_image_np.shape))
        images_np = np.concatenate(images_np, axis=0)
        if tile:
            images_tiled = tile_images(images_np)
            return images_tiled
        else:
            return images_np

    if image_tensor.dim() == 2:
        image_tensor = image_tensor.unsqueeze(0)
    image_numpy = image_tensor.detach().cpu().float().numpy()
    if normalize:
        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0
    else:
        image_numpy = np.transpose(image_numpy, (1, 2, 0)) * 255.0
    image_numpy = np.clip(image_numpy, 0, 255)
    if image_numpy.shape[2] == 1:
        image_numpy = image_numpy[:, :, 0]
    return image_numpy.astype(imtype)


# Converts a one-hot tensor into a colorful label map
def tensor2label(label_tensor, n_label, imtype=np.uint8, tile=False):
    if label_tensor.dim() == 4:
        # transform each image in the batch
        images_np = []
        for b in range(label_tensor.size(0)):
            one_image = label_tensor[b]
            one_image_np = tensor2label(one_image, n_label, imtype)
            images_np.append(one_image_np.reshape(1, *one_image_np.shape))
        images_np = np.concatenate(images_np, axis=0)
        if tile:
            images_tiled = tile_images(images_np)
            return images_tiled
        else:
            images_np = images_np[0]
            return images_np

    if label_tensor.dim() == 1:
        return np.zeros((64, 64, 3), dtype=np.uint8)
    if n_label == 0:
        return tensor2im(label_tensor, imtype)
    label_tensor = label_tensor.cpu().float()
    if label_tensor.size()[0] > 1:
        label_tensor = label_tensor.max(0, keepdim=True)[1]
    label_tensor = Colorize(n_label)(label_tensor)
    label_numpy = np.transpose(label_tensor.numpy(), (1, 2, 0))
    result = label_numpy.astype(imtype)
    return result


def save_image(image_numpy, image_path, create_dir=False):
    if create_dir:
        os.makedirs(os.path.dirname(image_path), exist_ok=True)
    if len(image_numpy.shape) == 2:
        image_numpy = np.expand_dims(image_numpy, axis=2)
    if image_numpy.shape[2] == 1:
        image_numpy = np.repeat(image_numpy, 3, 2)
    image_pil = Image.fromarray(image_numpy)

    # save to png
    image_pil.save(image_path.replace('.jpg', '.png'))


def mkdirs(paths):
    if isinstance(paths, list) and not isinstance(paths, str):
        for path in paths:
            mkdir(path)
    else:
        mkdir(paths)


def mkdir(path):
    if not os.path.exists(path):
        os.makedirs(path)


def atoi(text):
    return int(text) if text.isdigit() else text


def natural_keys(text):
    '''
    alist.sort(key=natural_keys) sorts in human order
    http://nedbatchelder.com/blog/200712/human_sorting.html
    (See Toothy's implementation in the comments)
    '''
    return [atoi(c) for c in re.split('(\d+)', text)]


def natural_sort(items):
    items.sort(key=natural_keys)


def str2bool(v):
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')

def find_class_in_module(target_cls_name, module):
    target_cls_name = target_cls_name.replace('_', '').lower()
    clslib = importlib.import_module(module)
    cls = None
    for name, clsobj in clslib.__dict__.items():
        if name.lower() == target_cls_name:
            cls = clsobj

    if cls is None:
        print("In %s, there should be a class whose name matches %s in lowercase without underscore(_)" % (module, target_cls_name))
        exit(0)

    return cls


def save_network(net, label, epoch, opt):
    save_filename = '%s_net_%s.pth' % (epoch, label)
    save_path = os.path.join(opt.checkpoints_dir, opt.name, save_filename)
    torch.save(net.cpu().state_dict(), save_path)
    if len(opt.gpu_ids) and torch.cuda.is_available():
        net.cuda()
    else:
        net.cpu()



def load_network(net, label, epoch, opt):

    #path on sherlock
    path = '/scratch/users/fwkong/SharedData/Generators/2d200epoch3Dnorm'

    #Generator Name
    save_filename = '200_net_G.pth'
    save_path = os.path.join(path, save_filename)
    weights = torch.load(save_path)
    net.load_state_dict(weights)
    return net


###############################################################################
# Code from
# https://github.com/ycszen/pytorch-seg/blob/master/transform.py
# Modified so it complies with the Citscape label map colors
###############################################################################
def uint82bin(n, count=8):
    """returns the binary of integer n, count refers to amount of bits"""
    return ''.join([str((n >> y) & 1) for y in range(count - 1, -1, -1)])


def labelcolormap(N):
    if N == 35:  # cityscape
        cmap = np.array([(0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (111, 74, 0), (81, 0, 81),
                         (128, 64, 128), (244, 35, 232), (250, 170, 160), (230, 150, 140), (70, 70, 70), (102, 102, 156), (190, 153, 153),
                         (180, 165, 180), (150, 100, 100), (150, 120, 90), (153, 153, 153), (153, 153, 153), (250, 170, 30), (220, 220, 0),
                         (107, 142, 35), (152, 251, 152), (70, 130, 180), (220, 20, 60), (255, 0, 0), (0, 0, 142), (0, 0, 70),
                         (0, 60, 100), (0, 0, 90), (0, 0, 110), (0, 80, 100), (0, 0, 230), (119, 11, 32), (0, 0, 142)],
                        dtype=np.uint8)
    else:
        cmap = np.zeros((N, 3), dtype=np.uint8)
        for i in range(N):
            r, g, b = 0, 0, 0
            id = i + 1  # let's give 0 a color
            for j in range(7):
                str_id = uint82bin(id)
                r = r ^ (np.uint8(str_id[-1]) << (7 - j))
                g = g ^ (np.uint8(str_id[-2]) << (7 - j))
                b = b ^ (np.uint8(str_id[-3]) << (7 - j))
                id = id >> 3
            cmap[i, 0] = r
            cmap[i, 1] = g
            cmap[i, 2] = b

        if N == 182:  # COCO
            important_colors = {
                'sea': (54, 62, 167),
                'sky-other': (95, 219, 255),
                'tree': (140, 104, 47),
                'clouds': (170, 170, 170),
                'grass': (29, 195, 49)
            }
            for i in range(N):
                name = util.coco.id2label(i)
                if name in important_colors:
                    color = important_colors[name]
                    cmap[i] = np.array(list(color))

    return cmap


class Colorize(object):
    def __init__(self, n=35):
        self.cmap = labelcolormap(n)
        self.cmap = torch.from_numpy(self.cmap[:n])

    def __call__(self, gray_image):
        size = gray_image.size()
        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)

        for label in range(0, len(self.cmap)):
            mask = (label == gray_image[0]).cpu()
            color_image[0][mask] = self.cmap[label][0]
            color_image[1][mask] = self.cmap[label][1]
            color_image[2][mask] = self.cmap[label][2]

        return color_image





from scipy.ndimage import label
def remove_all_but_the_largest_connected_component(image: np.ndarray, for_which_classes: list, volume_per_voxel: float,
                                                   minimum_valid_object_size: dict = None):
    """
    removes all but the largest connected component, individually for each class
    -for_which_classes: can be None. Should be list of int. Can also be something like [(1, 2), 2, 4].
    Here (1, 2) will be treated as a joint region, not individual classes 
    -minimum_valid_object_size: Only objects larger than minimum_valid_object_size will be removed. Keys in
    minimum_valid_object_size must match entries in for_which_classes
    """
    if for_which_classes is None:
        for_which_classes = np.unique(image)
        for_which_classes = for_which_classes[for_which_classes > 0]

    assert 0 not in for_which_classes, "cannot remove background"
    largest_removed = {}
    kept_size = {}
    for c in for_which_classes:
        if isinstance(c, (list, tuple)):
            c = tuple(c)  # otherwise it cant be used as key in the dict
            mask = np.zeros_like(image, dtype=bool)
            for cl in c:
                mask[image == cl] = True
        else:
            mask = image == c
        # get labelmap and number of objects
        lmap, num_objects = label(mask.astype(int))

        # collect object sizes
        object_sizes = {}
        for object_id in range(1, num_objects + 1):
            object_sizes[object_id] = (lmap == object_id).sum() * volume_per_voxel

        largest_removed[c] = None
        kept_size[c] = None

        if num_objects > 0:
            # always keep the largest object
            maximum_size = max(object_sizes.values())
            kept_size[c] = maximum_size

            for object_id in range(1, num_objects + 1):
                # only remove objects that are not the largest
                if object_sizes[object_id] != maximum_size:
                    # only remove objects that are smaller than minimum_valid_object_size
                    remove = True
                    if minimum_valid_object_size is not None:
                        remove = object_sizes[object_id] < minimum_valid_object_size[c]
                    if remove:
                        image[(lmap == object_id) & mask] = 0
                        if largest_removed[c] is None:
                            largest_removed[c] = object_sizes[object_id]
                        else:
                            largest_removed[c] = max(largest_removed[c], object_sizes[object_id])
    return image, largest_removed, kept_size




def save_as_pickle(data, file_path):
    """
    Save data as a Pickle file.
    """
    with open(file_path, 'wb') as f:
        pickle.dump(data, f)

#Fanwei's code in CHD - datasets - create_sdfdataset.py
def resample_image(source, target, order=1):
    if order==1:
        interp = sitk.sitkLinear
    else:
        interp = sitk.sitkNearestNeighbor
    source =  sitk.GetImageFromArray(source)
    source = sitk.Resample(source, target.GetSize(),
                             sitk.Transform(),
                             interp,
                             target.GetOrigin(),
                             target.GetSpacing(),
                             target.GetDirection(),
                             0,
                             source.GetPixelID())
    return source

def save_as_resized_pickle(tensor, pickle_file_path, target):
    """
    Resize the input tensor from 512x512x221 to 128x128x128 and save as a Pickle file. 
    """
    # Convert PyTorch tensor to NumPy array
    tensor_np = tensor.detach().cpu().numpy()

    # Check if the dimensions are 512x512x221
    if tensor_np.shape != (512, 512, 221):
        print(f"Unexpected dimensions: {tensor_np.shape}")
        return
        # Save tensor_np as a pickle file

    #transform np to pkl type
    # Rescale to 128x128x128
    resized_data = resample_image(tensor_np, target, order=1)

    # Save as Pickle file
    save_as_pickle(resized_data, pickle_file_path)

    print(f"Resized data saved to {pickle_file_path}")
    del tensor_np
    del resized_data
    gc.collect()

